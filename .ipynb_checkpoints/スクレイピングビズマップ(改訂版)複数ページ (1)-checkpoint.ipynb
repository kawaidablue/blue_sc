{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e613d6-3a8e-4b7f-8328-6155b7c8a3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLリストを urlsds.csv に保存しました。\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# SeleniumのWebDriverをセットアップ\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# 初期ページにアクセス\n",
    "base_url = \"https://biz-maps.com/s/cities/131041/m-inds/512\"\n",
    "driver.get(base_url)\n",
    "\n",
    "# URLを格納するリスト\n",
    "urls = []\n",
    "\n",
    "# ページを移動するためのループ\n",
    "for _ in range(2): #ページ数\n",
    "    try:\n",
    "        # 現在のURLを取得してリストに追加\n",
    "        current_url = driver.current_url\n",
    "        urls.append(current_url)\n",
    "\n",
    "        # 次のページリンクを探してクリック\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@rel='next']\")\n",
    "        next_button.click()\n",
    "        time.sleep(3)  # ページが完全にロードされるのを待つ\n",
    "    except Exception as e:\n",
    "        print(\"次のページが見つかりません:\", e)\n",
    "        break\n",
    "\n",
    "# WebDriverを終了\n",
    "driver.quit()\n",
    "\n",
    "# CSVファイルに保存\n",
    "csv_filename = \"urlsds.csv\"\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"URL\"])  # ヘッダー行\n",
    "    for url in urls:\n",
    "        writer.writerow([url])\n",
    "\n",
    "print(f\"URLリストを {csv_filename} に保存しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fb7ef1-d0e1-4316-9a0e-acc7998707d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL: https://biz-maps.com/s/cities/131041/m-inds/512\n",
      "Processing URL: https://biz-maps.com/s/cities/131041/m-inds/512?city%5B0%5D=131041&mid_industry%5B0%5D=512&ph=%25242y%252405%2524%252FmDYcz.znctbzLmKNi0ZaetolXAG6EzOcsglkD6dETs3QNt.JD7oy&page=2\n",
      "Data saved to companykokkoo_data.csv\n",
      "Total processing time: 110.13 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# 開始時刻の記録\n",
    "start_time = time.time()\n",
    "\n",
    "# 入力ファイル名と出力ファイル名\n",
    "input_csv = \"urlsds.csv\"\n",
    "output_csv = \"companykokkoo_data.csv\"\n",
    "\n",
    "# ヘッダー情報を指定\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# 入力CSVからURLを読み込む\n",
    "urls = []\n",
    "with open(input_csv, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # ヘッダーをスキップ\n",
    "    urls = [row[0] for row in reader]\n",
    "\n",
    "# データを格納するリスト\n",
    "all_company_names = []\n",
    "all_company_urls = []\n",
    "all_extracted_links = []\n",
    "\n",
    "# 各URLを訪問してデータを抽出\n",
    "for url in urls:\n",
    "    print(f\"Processing URL: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            # 会社名とURLを抽出\n",
    "            company_names = []\n",
    "            company_urls = []\n",
    "            extracted_links = []\n",
    "            \n",
    "            companies = soup.find_all(\"div\", class_=\"results__name\")\n",
    "            for company in companies:\n",
    "                name = company.text.strip()\n",
    "                parent = company.find_parent(\"a\")\n",
    "                company_url = parent.get(\"href\") if parent else None\n",
    "                if company_url and \"http\" not in company_url:\n",
    "                    company_url = \"https://biz-maps.com\" + company_url\n",
    "                \n",
    "                # 各企業のページを訪問して特定リンクを取得\n",
    "                if company_url:\n",
    "                    try:\n",
    "                        company_response = requests.get(company_url, headers=headers)\n",
    "                        if company_response.status_code == 200:\n",
    "                            company_soup = BeautifulSoup(company_response.content, \"html.parser\")\n",
    "                            specific_links = company_soup.find_all(\"a\", href=True)\n",
    "                            external_links = [\n",
    "                                link.get(\"href\")\n",
    "                                for link in specific_links\n",
    "                                if \"http\" in link.get(\"href\") and \"biz-maps.com\" not in link.get(\"href\")\n",
    "                            ]\n",
    "                            \n",
    "                            external_links = [\n",
    "                                \"\" if link == \"https://www.hifcorp.co.jp/\" else link\n",
    "                                for link in external_links\n",
    "                            ]\n",
    "                            \n",
    "                            link_text = external_links[0] if external_links else \"None\"\n",
    "                        else:\n",
    "                            link_text = \"Failed to fetch company page\"\n",
    "                    except Exception as e:\n",
    "                        link_text = f\"Error: {e}\"\n",
    "                else:\n",
    "                    link_text = \"None\"\n",
    "                \n",
    "                company_names.append(name)\n",
    "                company_urls.append(company_url)\n",
    "                extracted_links.append(link_text)\n",
    "            \n",
    "                time.sleep(2)  # サーバー負荷軽減のため遅延を追加\n",
    "            \n",
    "            # URLごとのデータを統合\n",
    "            all_company_names.extend(company_names)\n",
    "            all_company_urls.extend(company_urls)\n",
    "            all_extracted_links.extend(extracted_links)\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# 全データをCSV形式で保存\n",
    "data = {\n",
    "    \"企業名\": all_company_names,\n",
    "    \"企業URL\": all_extracted_links\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 終了時刻の記録と経過時間の計算\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Data saved to {output_csv}\")\n",
    "print(f\"Total processing time: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
